### å®æˆ˜ - AIç§‘ç ”åŠ©æ‰‹
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/139c75addae84c58a82ac54357688f71.png#pic_center)

é¡¹ç›®ä¸»è¦åŒ…å«ä¸€ä¸ªStreamlitå¼€å‘çš„å®¢æˆ·ç«¯ï¼Œä»¥åŠä¸€ä¸ªéƒ¨ç½²å¥½çš„æµªæ½®æºå¤§æ¨¡å‹çš„æœåŠ¡ç«¯ã€‚â€‹
- å®¢æˆ·ç«¯æ¥æ”¶åˆ°ç”¨æˆ·ä¸Šä¼ çš„PDFåï¼Œå‘é€åˆ°æœåŠ¡ç«¯ã€‚æœåŠ¡ç«¯é¦–å…ˆå®ŒæˆPDFå†…å®¹è§£æï¼Œç„¶åæ‹¼æ¥æ‘˜è¦Promptå¹¶è¾“å…¥æºå¤§æ¨¡å‹ï¼Œå¾—åˆ°æ¨¡å‹è¾“å‡ºç»“æœåï¼Œè¿”å›ç»™å®¢æˆ·ç«¯å¹¶å±•ç¤ºç»™ç”¨æˆ·ã€‚â€‹
- å¦‚æœç”¨æˆ·æ¥ä¸‹æ¥è¿›è¡Œæé—®ï¼Œå®¢æˆ·ç«¯å°†ç”¨æˆ·è¯·æ±‚å‘é€åˆ°æœåŠ¡ç«¯ï¼ŒæœåŠ¡ç«¯è¿›è¡ŒEmbeddingå’ŒFaissæ£€ç´¢ï¼Œç„¶åå°†æ£€ç´¢åˆ°çš„chunksä¸ç”¨æˆ·è¯·æ±‚æ‹¼æ¥æˆPromptå¹¶è¾“å…¥åˆ°æºå¤§æ¨¡å‹ï¼Œå¾—åˆ°æ¨¡å‹è¾“å‡ºç»“æœåï¼Œè¿”å›ç»™å®¢æˆ·ç«¯è¿›è¡Œç»“æ„åŒ–ï¼Œç„¶åå±•ç¤ºç»™ç”¨æˆ·ã€‚

#### é¡¹ç›®ç»“æ„
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/b60ac1a9f3a84eeb81649234d45e4d7f.png#pic_center)
ä¸»æ¨¡å—ï¼š`main.py`
```python
# å¯¼å…¥å¿…è¦çš„åº“å’Œæ¨¡å—
from langchain_community.document_loaders import PyPDFLoader  # ç”¨äºåŠ è½½PDFæ–‡ä»¶çš„åŠ è½½å™¨
from common import constants  # å¯¼å…¥å¸¸é‡é…ç½®
import streamlit as st  # å¯¼å…¥Streamlitåº“ï¼Œç”¨äºæ„å»ºWebç•Œé¢

from llm.yuan2_llm import Yuan2_LLM  # å¯¼å…¥è‡ªå®šä¹‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ç±»
from langchain_huggingface import HuggingFaceEmbeddings  # å¯¼å…¥HuggingFaceåµŒå…¥æ¨¡å‹
# å¯¼å…¥æç¤ºæ¨¡æ¿ç±»
from prompts.chatbot_template import ChatBot
from prompts.summarizer_template import Summarizer

# å®šä¹‰æ¨¡å‹è·¯å¾„
model_path = constants.MODEL_PATH  # ä»å¸¸é‡é…ç½®ä¸­è·å–æ¨¡å‹è·¯å¾„

# å®šä¹‰å‘é‡æ¨¡å‹è·¯å¾„
embedding_model_path = constants.EMBED_MODEL_PATH  # ä»å¸¸é‡é…ç½®ä¸­è·å–å‘é‡æ¨¡å‹è·¯å¾„


# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºè·å–llmå’Œembeddings
@st.cache_resource  # ä½¿ç”¨Streamlitçš„ç¼“å­˜è£…é¥°å™¨æ¥ç¼“å­˜å‡½æ•°çš„ç»“æœ
def get_models():
    llm = Yuan2_LLM(model_path)  # åˆ›å»ºYuan2_LLMå®ä¾‹

    # å®šä¹‰æ¨¡å‹å’Œç¼–ç çš„å‚æ•°
    model_kwargs = {'device': 'cuda'}
    encode_kwargs = {'normalize_embeddings': True}  # è®¾ç½®ä¸ºTrueä»¥è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    embeddings = HuggingFaceEmbeddings(
        model_name=embedding_model_path,  # å‘é‡æ¨¡å‹çš„åç§°æˆ–è·¯å¾„
        model_kwargs=model_kwargs,  # æ¨¡å‹å‚æ•°
        encode_kwargs=encode_kwargs,  # ç¼–ç å‚æ•°
    )
    return llm, embeddings  # è¿”å›åˆ›å»ºçš„LLMå’ŒåµŒå…¥æ¨¡å‹å®ä¾‹


def main():
    # åˆ›å»ºä¸€ä¸ªæ ‡é¢˜
    st.title('ğŸ’¬ Yuan2.0 AIç§‘ç ”åŠ©æ‰‹')  # è®¾ç½®Streamlitåº”ç”¨çš„æ ‡é¢˜

    # è·å–llmå’Œembeddings
    llm, embeddings = get_models()  # è°ƒç”¨get_modelså‡½æ•°è·å–æ¨¡å‹å®ä¾‹

    # åˆå§‹åŒ–summarizer
    summarizer = Summarizer(llm)  # åˆ›å»ºSummarizerå®ä¾‹ç”¨äºç”Ÿæˆæ–‡æœ¬æ‘˜è¦

    # åˆå§‹åŒ–ChatBot
    chatbot = ChatBot(llm, embeddings)  # åˆ›å»ºChatBotå®ä¾‹ç”¨äºå›ç­”é—®é¢˜

    # ä¸Šä¼ pdf
    uploaded_file = st.file_uploader("Upload your PDF", type='pdf')  # åˆ›å»ºæ–‡ä»¶ä¸Šä¼ å™¨ï¼Œå…è®¸ç”¨æˆ·ä¸Šä¼ PDFæ–‡ä»¶

    if uploaded_file:
        # åŠ è½½ä¸Šä¼ PDFçš„å†…å®¹
        file_content = uploaded_file.read()  # è¯»å–ä¸Šä¼ çš„æ–‡ä»¶å†…å®¹

        # å†™å…¥ä¸´æ—¶æ–‡ä»¶
        temp_file_path = "temp.pdf"  # å®šä¹‰ä¸´æ—¶æ–‡ä»¶è·¯å¾„
        with open(temp_file_path, "wb") as temp_file:
            temp_file.write(file_content)  # å°†æ–‡ä»¶å†…å®¹å†™å…¥ä¸´æ—¶æ–‡ä»¶

        # åŠ è½½ä¸´æ—¶æ–‡ä»¶ä¸­çš„å†…å®¹
        loader = PyPDFLoader(temp_file_path)  # åˆ›å»ºPDFåŠ è½½å™¨å®ä¾‹
        docs = loader.load()  # ä½¿ç”¨åŠ è½½å™¨åŠ è½½æ–‡æ¡£å†…å®¹

        st.chat_message("assistant").write(f"æ­£åœ¨ç”Ÿæˆè®ºæ–‡æ¦‚æ‹¬ï¼Œè¯·ç¨å€™...")  # åœ¨Streamlitç•Œé¢ä¸Šæ˜¾ç¤ºæ¶ˆæ¯

        # ç”Ÿæˆæ¦‚æ‹¬
        summary = summarizer.summarize(docs)  # è°ƒç”¨summarizerçš„summarizeæ–¹æ³•ç”Ÿæˆæ‘˜è¦

        # åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºæ¨¡å‹çš„è¾“å‡º
        st.chat_message("assistant").write(summary)  # æ˜¾ç¤ºç”Ÿæˆçš„æ‘˜è¦

        # æ¥æ”¶ç”¨æˆ·é—®é¢˜
        if query := st.text_input("Ask questions about your PDF file"):  # åˆ›å»ºæ–‡æœ¬è¾“å…¥æ¡†ï¼Œå…è®¸ç”¨æˆ·è¾“å…¥é—®é¢˜
            # æ£€ç´¢ + ç”Ÿæˆå›å¤
            chunks, response = chatbot.run(docs, query)  # è°ƒç”¨chatbotçš„runæ–¹æ³•è¿›è¡Œæ£€ç´¢å’Œç”Ÿæˆå›ç­”

            # åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºæ¨¡å‹çš„è¾“å‡º
            st.chat_message("assistant").write(f"æ­£åœ¨æ£€ç´¢ç›¸å…³ä¿¡æ¯ï¼Œè¯·ç¨å€™...")  # æ˜¾ç¤ºæ£€ç´¢ä¿¡æ¯çš„æ¶ˆæ¯
            st.chat_message("assistant").write(chunks)  # æ˜¾ç¤ºæ£€ç´¢åˆ°çš„æ–‡æ¡£ç‰‡æ®µ

            st.chat_message("assistant").write(f"æ­£åœ¨ç”Ÿæˆå›å¤ï¼Œè¯·ç¨å€™...")  # æ˜¾ç¤ºç”Ÿæˆå›ç­”çš„æ¶ˆæ¯
            st.chat_message("assistant").write(response)  # æ˜¾ç¤ºç”Ÿæˆçš„å›ç­”


if __name__ == '__main__':
    main()  # å¦‚æœæ˜¯ä¸»ç¨‹åºï¼Œåˆ™è°ƒç”¨mainå‡½æ•°è¿è¡Œåº”ç”¨
```
æç¤ºè¯æ¨¡å—ï¼š

`chatbot_template.py`

```python
from langchain_core.prompts import PromptTemplate
from langchain_text_splitters import RecursiveCharacterTextSplitter  # å¯¼å…¥æ–‡æœ¬åˆ†å‰²å™¨
from langchain.chains.question_answering import load_qa_chain  # å¯¼å…¥load_qa_chainï¼Œç”¨äºåŠ è½½é—®ç­”é“¾
from langchain_community.vectorstores import FAISS
# å®šä¹‰èŠå¤©æœºå™¨äººæ¨¡æ¿
chatbot_template = '''
å‡è®¾ä½ æ˜¯ä¸€ä¸ªAIç§‘ç ”åŠ©æ‰‹ï¼Œè¯·åŸºäºèƒŒæ™¯ï¼Œç®€è¦å›ç­”é—®é¢˜ã€‚

èƒŒæ™¯ï¼š
{context}

é—®é¢˜ï¼š
{question}
'''.strip()


# å®šä¹‰ChatBotç±»
class ChatBot:
    """
    ChatBotç±»ç”¨äºå¤„ç†ç”¨æˆ·æé—®ï¼Œå¹¶åŸºäºæ–‡æ¡£å†…å®¹ç”Ÿæˆå›ç­”ã€‚
    """

    def __init__(self, llm, embeddings):
        self.prompt = PromptTemplate(
            input_variables=["text"],
            template=chatbot_template
        )  # å®šä¹‰èŠå¤©æœºå™¨äººæç¤ºæ¨¡æ¿
        self.chain = load_qa_chain(llm=llm, chain_type="stuff", prompt=self.prompt)  # åŠ è½½é—®ç­”é“¾
        self.embeddings = embeddings  # åµŒå…¥æ¨¡å‹ï¼Œç”¨äºæ–‡æ¡£å‘é‡åŒ–

        # åŠ è½½æ–‡æœ¬åˆ†å‰²å™¨ï¼Œç”¨äºå°†é•¿æ–‡æœ¬åˆ‡åˆ†æˆå°å—
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=450,
            chunk_overlap=10,
            length_function=len
        )

    def run(self, docs, query):
        """
        å¤„ç†ç”¨æˆ·æé—®ï¼Œç”Ÿæˆå›ç­”ã€‚

        :param docs: æ–‡æ¡£åˆ—è¡¨ï¼Œæ¯ä¸ªæ–‡æ¡£åŒ…å«page_contentå±æ€§
        :param query: ç”¨æˆ·çš„æé—®
        :return: æ£€ç´¢åˆ°çš„æ–‡æ¡£ç‰‡æ®µå’Œç”Ÿæˆçš„å›ç­”
        """
        # è¯»å–æ‰€æœ‰æ–‡æ¡£å†…å®¹
        text = ''.join([doc.page_content for doc in docs])

        # ä½¿ç”¨æ–‡æœ¬åˆ†å‰²å™¨åˆ‡åˆ†æˆchunks
        all_chunks = self.text_splitter.split_text(text=text)

        # å°†æ–‡æœ¬chunksè½¬æ¢ä¸ºå‘é‡å¹¶å­˜å‚¨
        VectorStore = FAISS.from_texts(all_chunks, embedding=self.embeddings)

        # æ£€ç´¢ä¸æé—®æœ€ç›¸ä¼¼çš„chunks
        chunks = VectorStore.similarity_search(query=query, k=1)

        # ä½¿ç”¨é—®ç­”é“¾ç”Ÿæˆå›ç­”
        response = self.chain.run(input_documents=chunks, question=query)

        return chunks, response  # è¿”å›æ£€ç´¢åˆ°çš„æ–‡æ¡£ç‰‡æ®µå’Œç”Ÿæˆçš„å›ç­”

```
`summarizer_template.py`
```python
# å¯¼å…¥å¿…è¦çš„åº“å’Œæ¨¡å—
from langchain.chains.llm import LLMChain  # å¯¼å…¥LLMChainï¼Œç”¨äºæ„å»ºåŸºäºLLMçš„ç”Ÿæˆé“¾
from langchain_core.prompts import PromptTemplate  # å¯¼å…¥PromptTemplateï¼Œç”¨äºæ„å»ºæç¤ºæ¨¡æ¿


# å®šä¹‰æ‘˜è¦æ¨¡æ¿
summarizer_template = """
å‡è®¾ä½ æ˜¯ä¸€ä¸ªAIç§‘ç ”åŠ©æ‰‹ï¼Œè¯·ç”¨ä¸€æ®µè¯æ¦‚æ‹¬ä¸‹é¢æ–‡ç« çš„ä¸»è¦å†…å®¹ï¼Œ200å­—å·¦å³ã€‚

{text}
"""

# å®šä¹‰Summarizerç±»
class Summarizer:
    """
    Summarizerç±»ç”¨äºå°†é•¿æ–‡æœ¬å†…å®¹å‹ç¼©æˆç®€çŸ­çš„æ‘˜è¦ã€‚
    """

    def __init__(self, llm):
        self.llm = llm  # LLMå®ä¾‹ï¼Œç”¨äºç”Ÿæˆæ–‡æœ¬
        self.prompt = PromptTemplate(
            input_variables=["text"],
            template=summarizer_template
        )  # å®šä¹‰æ‘˜è¦æç¤ºæ¨¡æ¿
        self.chain = LLMChain(llm=self.llm, prompt=self.prompt)  # åˆ›å»ºLLMChainå®ä¾‹ï¼Œç”¨äºç”Ÿæˆæ‘˜è¦

    def summarize(self, docs):
        """
        ä»æ–‡æ¡£ä¸­ç”Ÿæˆæ‘˜è¦ã€‚

        :param docs: æ–‡æ¡£åˆ—è¡¨ï¼Œå…¶ä¸­æ¯ä¸ªæ–‡æ¡£åŒ…å«page_contentå±æ€§
        :return: ç”Ÿæˆçš„æ‘˜è¦æ–‡æœ¬
        """
        # ä»ç¬¬ä¸€é¡µä¸­è·å–æ‘˜è¦å†…å®¹ï¼Œå‡è®¾æ‘˜è¦ä½äº'ABSTRACT'å’Œ'KEY WORDS'ä¹‹é—´
        content = docs[0].page_content.split('ABSTRACT')[1].split('KEY WORDS')[0]
        summary = self.chain.run(content)  # ä½¿ç”¨LLMChainç”Ÿæˆæ‘˜è¦
        return summary  # è¿”å›ç”Ÿæˆçš„æ‘˜è¦
```
æºå¤§æ¨¡å‹æ¨¡å—ï¼š
`yuan2_llm.py`
```python
# å¯¼å…¥å¿…è¦çš„åº“
from typing import List, Optional, Any
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# å¯¼å…¥å¸¸é‡é…ç½®
from common import constants

# å¯¼å…¥LLMåŸºç±»
from langchain.llms.base import LLM
from langchain.callbacks.manager import CallbackManagerForLLMRun

# å®šä¹‰æ¨¡å‹è·¯å¾„
model_path = constants.MODEL_PATH

# å®šä¹‰æ¨¡å‹æ•°æ®ç±»å‹
torch_dtype = torch.bfloat16


# å®šä¹‰æºå¤§æ¨¡å‹ç±»
class Yuan2_LLM(LLM):
    """
    YUAN2_LLMç±»ç”¨äºåŠ è½½å’Œä½¿ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚
    å®ƒç»§æ‰¿è‡ªlangchainçš„LLMåŸºç±»ï¼Œå¹¶å®ç°äº†è‡ªå·±çš„_callæ–¹æ³•æ¥ç”Ÿæˆæ–‡æœ¬ã€‚
    """

    # ç±»å˜é‡ï¼Œç”¨äºå­˜å‚¨åˆ†è¯å™¨å’Œæ¨¡å‹å®ä¾‹
    tokenizer: AutoTokenizer = None
    model: AutoModelForCausalLM = None

    def __init__(self, model_path: str):
        super().__init__()

        # åŠ è½½é¢„è®­ç»ƒçš„åˆ†è¯å™¨å’Œæ¨¡å‹
        print("Creating tokenizer...")
        self.tokenizer = AutoTokenizer.from_pretrained(model_path, add_eos_token=False, add_bos_token=False,
                                                       eos_token='<eod>')
        # æ·»åŠ ç‰¹æ®Šæ ‡è®°
        self.tokenizer.add_tokens(
            ['<sep>', '<pad>', '<mask>', '<predict>', '<FIM_SUFFIX>', '<FIM_PREFIX>', '<FIM_MIDDLE>', '<commit_before>',
             '<commit_msg>', '<commit_after>', '<jupyter_start>', '<jupyter_text>', '<jupyter_code>',
             '<jupyter_output>', '<empty_output>'], special_tokens=True)

        print("Creating model...")
        self.model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch_dtype,
                                                          trust_remote_code=True).cuda()

    def _call(
            self,
            prompt: str,
            stop: Optional[List[str]] = None,
            run_manager: Optional[CallbackManagerForLLMRun] = None,
            **kwargs: Any,
    ) -> str:
        """
        ç”Ÿæˆæ–‡æœ¬çš„æ–¹æ³•ï¼Œæ ¹æ®è¾“å…¥çš„promptç”Ÿæˆå“åº”ã€‚

        :param prompt: è¾“å…¥çš„æ–‡æœ¬æç¤º
        :param stop: åœæ­¢ç”Ÿæˆçš„æ ‡è®°åˆ—è¡¨
        :param run_manager: è¿è¡Œç®¡ç†å™¨ï¼Œç”¨äºç›‘æ§å’Œç®¡ç†ç”Ÿæˆè¿‡ç¨‹
        :param kwargs: å…¶ä»–å¯é€‰å‚æ•°
        :return: ç”Ÿæˆçš„æ–‡æœ¬å“åº”
        """
        prompt = prompt.strip()
        prompt += "<sep>"
        inputs = self.tokenizer(prompt, return_tensors="pt")["input_ids"].cuda()
        outputs = self.model.generate(inputs, do_sample=False, max_length=4096)
        output = self.tokenizer.decode(outputs[0])
        response = output.split("<sep>")[-1].split("<eod>")[0]

        return response

    @property
    def _llm_type(self) -> str:
        """
        è¿”å›æ¨¡å‹çš„ç±»å‹æ ‡è¯†ã€‚

        :return: æ¨¡å‹ç±»å‹å­—ç¬¦ä¸²
        """
        return "Yuan2_LLM"
```
#### è¿è¡Œæ•ˆæœ
```shell
bash run.sh
```
å¯åŠ¨è„šæœ¬ï¼š`run.sh`
```shell
streamlit run main.py --server.address 127.0.0.1 --server.port 6006
```
è®ºæ–‡æ¦‚æ‹¬ï¼š
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/b90be2cc1a09497a97b688026c8255d8.png#pic_center)
```
What kind of attention architecture is LFA?
```
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/e130415e604e4f05be3f84586c374353.png#pic_center)